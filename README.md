# DA_in_GameDev_5

Отчет по лабораторной работе #5 выполнил(а):
- Басырова Алина Радмировна
- РИ220942
  
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Задание 2.
- Задание 3.
- Выводы.

## Цель работы
разработать баланс для десяти уровней игры Dragon Picker

Ход работы:

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции” и сделать выводы о том, как он влияет на обучение модели.

```cs
public override void OnActionReceived(ActionBuffers actionBuffers)
{
Vector3 controlSignal = Vector3.zero;
controlSignal.x = actionBuffers.ContinuousActions[0];
controlSignal.z = actionBuffers.ContinuousActions[1];
rBody.AddForce(controlSignal * forceMultiplier);

float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition);

if(distanceToTarget < 1.42f)
{
SetReward(1.0f);
EndEpisode();
}
else if (this.transform.localPosition.y < 0)
{
EndEpisode();
}
}
```

Коэффициент корреляции равен 1.42f. Он определяет, насколько близко объект должен находиться к цели, чтобы считаться успешным. Чем он больше, тем быстрее будет обучаться модель, но ее точность будет снижаться. При уменьшении коэффициент растет точность, но обучение занимает намного больше времени.

## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

- learning_rate. Параметр устанавливает скорость обучения. Если learning rate слишком большой, модель может расходиться. Если слишком мал, то обучение может быть слишком медленным.
- num_epoch. Параметр определяет количество эпох обучения. Чем больше эпох, тем выше точность модели, но обучение занимает больше времени.
- epsilon - Определяет максимально возможное расхождение между прошлой и настоящей версией агента. Чем меньше значение, тем медленнее будет обучение, но зато оно будет стабильным. Более выскоие значения обеспечивает быстрое, но нестабильное обучение.

## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

1 ML-Agent может быть использован для написания поведения врагов, которые преследуют игрока, либо для npc, которые должны следовать определенному маршруту. 
2 ML-Agent может использоваться

Когда задача требует адаптации к изменениям: ML-агенты могут обучаться на новых данных и адаптироваться к изменяющейся среде, что может быть сложно реализовать с помощью программного решения.
Когда требуется оптимизация: ML-агенты могут использоваться для оптимизации сложных задач, таких как управление процессами или поиск оптимальных стратегий.

## Выводы

- В ходе лабороторной работы я проанализировала игру Dragon Picker и добавила в нее еще 9 уровней с возрастанием сложности (в итоге 10 уровней), написан скрипт на Python, который заполняет таблицу данными, необходимыми для создания сцен, и визуализирует эти данные.

| Plugin | README |
| ------ | ------ |
| GitHub | [plugins/github/README.md][PlGh] |
| VS Code | [plugins/vscode/README.md][PlGh] |
| Unity | [plugins/unity/README.md][PlMe] |
| Jupiter Notebook | [plugins/jupiternotebook/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
